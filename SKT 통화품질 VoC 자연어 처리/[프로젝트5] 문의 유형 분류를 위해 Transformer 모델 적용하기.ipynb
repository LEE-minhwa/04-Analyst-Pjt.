{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [프로젝트5] 문의 유형 분류를 위해 Transformer 모델 적용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BG_0zjG13zZy"
   },
   "source": [
    "\n",
    "## 프로젝트 목표\n",
    "---\n",
    "- Transformer Encoder 모델 구성\n",
    "- Transformer Decoder 모델 구성\n",
    "- Transformer Encoder를 활용한 분류 모델 학습 및 분석 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pS7RvUWi5vJe"
   },
   "source": [
    "## 프로젝트 목차\n",
    "---\n",
    "\n",
    "1. **Transformer Encoder 모델 구성:** Transformer Encoder 모델을 구성합니다.\n",
    "\n",
    "2. **Transformer Decoder 모델 구성:** Transformer Decoder 모델을 구성합니다.\n",
    "\n",
    "3. **Transformer Encoder를 활용한 분류 모델 학습 및 분석 :** Transformer Encoder를 활용하여 문의 유형 분류 모델을 만들고 학습합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-rxhtJI5_D2"
   },
   "source": [
    "## 프로젝트 개요\n",
    "---\n",
    "\n",
    "프로젝트 4에서 LSTM 모델을 구성하고 Attention 모듈을 추가하여 학습하여 분류 모델을 만들었습니다. 이번 프로젝트에서는 self-attention이 핵심 모듈인 transformer를 구성하여 봅니다. Transformer Encoder를 활용하여 분류 모델 학습을 진행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 전처리\n",
    "\n",
    "---\n",
    "\n",
    "### 1.1. 라이브러리 및 데이터 불러오기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "프로젝트 1에서 사용한 데이터와 모델 학습을 위해 필요한 라이브러리를 불러옵니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (1.10.0)\n",
      "Requirement already satisfied: torchtext==0.11.0 in /opt/conda/lib/python3.8/site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torchtext==0.11.0) (1.19.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from torchtext==0.11.0) (2.25.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from torchtext==0.11.0) (4.55.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->torchtext==0.11.0) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->torchtext==0.11.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests->torchtext==0.11.0) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->torchtext==0.11.0) (1.26.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchtext==0.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./01_data.csv', encoding='cp949')\n",
    "texts = data['메모'].tolist() # 자연어 데이터를 리스트 형식으로 변환합니다\n",
    "label_list = data['상담유형3_GT'].unique().tolist()\n",
    "labels = data['상담유형3_GT'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(text):\n",
    "    # 정제: 한글, 공백 제외한 문자 제거\n",
    "    text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_clean = []\n",
    "for i in range(len(texts)):\n",
    "    text_clean = cleaning(texts[i])\n",
    "    texts_clean.append(text_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 데이터와 테스트 데이터를 구분합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = int(0.8*len(texts_clean))\n",
    "\n",
    "texts_labels = list(zip(texts_clean,labels))\n",
    "random.shuffle(texts_labels)\n",
    "texts_clean, labels = zip(*texts_labels)\n",
    "\n",
    "train_texts = texts_clean[:num_train]\n",
    "train_labels = labels[:num_train]\n",
    "\n",
    "test_texts = texts_clean[num_train:]\n",
    "test_labels = labels[num_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame({'text': train_texts,\n",
    "                          'label': train_labels})\n",
    "test_data = pd.DataFrame({'text': test_texts,\n",
    "                          'label': test_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('./train_data.csv',index=False)\n",
    "test_data.to_csv('./test_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. 데이터 전처리\n",
    "---\n",
    "\n",
    "List 형태로 저장되어 있는 데이터와 라벨을 torch 모델에 적용할 수 있도록 전처리합니다. 이때, torchtext 라이브러리를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Okt()\n",
    "\n",
    "TEXT = torchtext.legacy.data.Field(tokenize=tokenizer.morphs,\n",
    "                 include_lengths=True)\n",
    "\n",
    "LABEL = torchtext.legacy.data.LabelField(dtype=torch.long)\n",
    "\n",
    "fields = {'text': ('text', TEXT), 'label': ('label', LABEL)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train, validation, test 데이터를 구분지어 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = torchtext.legacy.data.TabularDataset.splits(\n",
    "                            path = './',\n",
    "                            train = 'train_data.csv',\n",
    "                            test = 'test_data.csv',\n",
    "                            format = 'csv',\n",
    "                            fields = fields,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_data.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자연어 데이터를 컴퓨터로 표현하기 위한 임베딩 벡터를 가져옵니다. 본 프로젝트에서는 한국어 임베딩이 있는 FastText 서브 워드 임베딩을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data,\n",
    "                 max_size = 10000,\n",
    "                 vectors = 'fasttext.simple.300d',\n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = torchtext.legacy.data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = batch_size,\n",
    "    sort_key = lambda x: len(x.text),\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformer Encoder 모델 구성\n",
    "\n",
    "---\n",
    "\n",
    "Transformer Encoder 모델을 torch로 구성합니다. Encoder 모듈은 아래와 같이 구성됩니다.\n",
    "\n",
    "\n",
    "* Transformer Encoder\n",
    "    * Positional Encoding\n",
    "    * Transformer Encoder Layer\n",
    "        * Multi-head (self) Attention\n",
    "            * Scaled-Dot Product Attention\n",
    "        * Feed-Forward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaled-Dot Product Attention 모듈을 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, temperature, dropout=0.1):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        attn_score = torch.matmul(q, k.transpose(2, 3)) / self.temperature\n",
    "        if mask is not None:\n",
    "            attn_score = attn_score.masked_fill(mask=mask, value=float('-inf'))\n",
    "        attn_dist = torch.softmax(attn_score, dim=-1)\n",
    "        attn = self.dropout(attn_dist)\n",
    "\n",
    "        attn_out = torch.matmul(attn, v)\n",
    "\n",
    "        return attn_out, attn_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-head Attention 모듈을 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, k_channels, v_channels, n_head=8, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.k_channels = k_channels\n",
    "        self.v_channels = v_channels\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.q_linear = nn.Linear(embedding_dim, n_head * k_channels)\n",
    "        self.k_linear = nn.Linear(embedding_dim, n_head * k_channels)\n",
    "        self.v_linear = nn.Linear(embedding_dim, n_head * v_channels)\n",
    "        self.attention = ScaledDotProductAttention(temperature=k_channels ** 0.5, dropout=dropout)\n",
    "        self.out_linear = nn.Linear(n_head * v_channels, embedding_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        b, q_len, k_len, v_len = q.size(0), q.size(1), k.size(1), v.size(1)\n",
    "\n",
    "        q = self.q_linear(q).view(b, q_len, self.n_head, self.k_channels).transpose(1, 2)\n",
    "        k = self.k_linear(k).view(b, k_len, self.n_head, self.k_channels).transpose(1, 2)\n",
    "        v = self.v_linear(v).view(b, v_len, self.n_head, self.v_channels).transpose(1, 2)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        out, attn = self.attention(q, k, v, mask=mask)\n",
    "        out = out.transpose(1, 2).contiguous().view(b, q_len, self.n_head * self.v_channels)\n",
    "        out = self.out_linear(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed-forward 모듈을 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, filter_size, hidden_size, dropout=0.1):\n",
    "        super(Feedforward, self).__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, filter_size, True)\n",
    "        self.fc2 = nn.Linear(filter_size, hidden_size, True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer Encoder Layer (block) 모듈을 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, filter_size, num_heads, dropout_rate):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(embedding_dim=embedding_dim, k_channels=embedding_dim//num_heads, v_channels=embedding_dim//num_heads, n_head=num_heads, dropout=dropout_rate)\n",
    "        self.attention_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "\n",
    "        self.feedforward = Feedforward(filter_size=filter_size, hidden_size=embedding_dim)\n",
    "        self.feedforward_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        attn_out, _ = self.attention(src, src, src, src_mask)\n",
    "        out = src + attn_out\n",
    "        out = self.attention_norm(out)\n",
    "        \n",
    "        ffn_out = self.feedforward(out)\n",
    "        out = out + ffn_out\n",
    "        out = self.feedforward_norm(out) \n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional Encoder 모듈을 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len=2000, dropout=0.1):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "\n",
    "        self.position_encoder = self.generate_encoder(embedding_dim, max_len)\n",
    "        self.position_encoder = self.position_encoder.unsqueeze(0)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def generate_encoder(self, embedding_dim, max_len):\n",
    "        pos = torch.arange(max_len).float().unsqueeze(1)\n",
    "\n",
    "        i = torch.arange(embedding_dim).float().unsqueeze(0)\n",
    "        angle_rates = 1 / torch.pow(10000, (2 * (i // 2)) / embedding_dim)\n",
    "\n",
    "        position_encoder = pos * angle_rates\n",
    "        position_encoder[:, 0::2] = torch.sin(position_encoder[:, 0::2])\n",
    "        position_encoder[:, 1::2] = torch.cos(position_encoder[:, 1::2])\n",
    "\n",
    "        return position_encoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.position_encoder[:, :x.size(1), :]\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 모듈을 기반으로 Transformer Encoder 모델을 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, filter_size, num_enc_layers, num_heads, dropout_rate):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoder(embedding_dim=embedding_dim, dropout=dropout_rate)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(embedding_dim, filter_size, num_heads, dropout_rate) for _ in range(num_enc_layers)])\n",
    "\n",
    "    def forward(self, src, src_mask=None, length=None):\n",
    "        out = self.pos_encoder(src)\n",
    "        \n",
    "        if length is not None:\n",
    "            src_mask = torch.ones(src.size(0), src.size(1))\n",
    "            for i in range(src.size(0)):\n",
    "                src_mask[i][:length[i]] = 0\n",
    "            src_mask = src_mask.bool()\n",
    "            src_mask = src_mask.unsqueeze(1)\n",
    "            \n",
    "        for layer in self.layers:\n",
    "            out = layer(out, src_mask)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformer Decoder 모델 구성\n",
    "\n",
    "---\n",
    "\n",
    "Transformer Decoder 모델을 torch로 구성합니다. Decoder 모듈은 아래와 같이 구성됩니다.\n",
    "\n",
    "\n",
    "* Transformer Decoder\n",
    "    * Transformer Decoder Layer\n",
    "        * Multi-head (self) Attention\n",
    "            * Scaled-Dot Product Attention\n",
    "        * Multi-head (encoder-decoder) Attebtion\n",
    "            * Scaled-Dot Product Attention\n",
    "        * Feed-Forward\n",
    "\n",
    "위와 같이 기존 구성한 attention, feed-forward 모듈로 구성할 수 있습니다. self_attention과 attention 모델이 forward 함수에서 어떤 입력값을 받는 지 유의깊게 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, input_size, filter_size, num_heads, dropout_rate):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(in_channels=input_size, k_channels=input_size//num_heads, v_channels=input_size//num_heads, n_head=num_heads, dropout=dropout_rate)\n",
    "        self.self_attention_norm = nn.LayerNorm(normalized_shape=input_size)\n",
    "\n",
    "        self.attention = MultiHeadAttention(in_channels=input_size, k_channels=input_size//num_heads, v_channels=input_size//num_heads, n_head=num_heads, dropout=dropout_rate)\n",
    "        self.attention_norm = nn.LayerNorm(normalized_shape=input_size)\n",
    "\n",
    "        self.feedforward = Feedforward(filter_size=filter_size, hidden_size=input_size, fc_option=fc_option)\n",
    "        self.feedforward_norm = nn.LayerNorm(normalized_shape=input_size)\n",
    "\n",
    "    def forward(self, src, tgt, mask=None):\n",
    "        self_attn_out, _ = self.self_attention(tgt, tgt, tgt, mask)\n",
    "        out = tgt + self_attn_out\n",
    "        out = self.self_attention_norm(out)\n",
    "        \n",
    "        attn_out, _ = self.attention(out, src, src)\n",
    "        out = out + attn_out\n",
    "        out = self.attention_norm(out)\n",
    "        \n",
    "        ffn_out = self.feedforward(out_norm)\n",
    "        out = out + ffn_out\n",
    "        out = self.feedforward_norm(out) \n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, input_size, filter_size, num_enc_layers, num_heads, dropout_rate):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([TransformerDecoderLayer(input_size, filter_size, num_heads, dropout_rate) for _ in range(num_enc_layers)])\n",
    "\n",
    "    def forward(self, src, tgt, mask=None):\n",
    "        out = tgt\n",
    "        for layer in self.layers:\n",
    "            out = layer(src, out, mask)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transformer Encoder을 통한 문의 유형 분류 문제\n",
    "\n",
    "---\n",
    "Transformer Encoder-Decoder 구조 자체는 입력 시퀀스와 출력 시퀀스가 있을 때 잘 활용될 수 있는 모델입니다. 분류 문제의 경우 입력 시퀀스의 정보로부터 유형을 찾아내면 되기 때문에 Transformer Encoder 구조만을 활용해도 충분합니다.\n",
    "\n",
    "따라서 이번 단원에서는 Transformer Encoder 구조를 활용하여 문의 유형 분류 모델을 구성하여 학습해보도록 하겠습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Transformer 분류 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, filter_size, vocab_size, target_size, num_enc_layers, num_heads, dropout_rate, pad_idx):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        # 단어 임베딩\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        # Transformer Encoder\n",
    "        self.tfm = TransformerEncoder(embedding_dim, filter_size, num_enc_layers, num_heads, dropout_rate)\n",
    "        # 분류자 (classifier)\n",
    "        self.fc = nn.Linear(embedding_dim, target_size)\n",
    "    \n",
    "    def forward(self, text, text_length):\n",
    "        embeds = self.word_embeddings(text)\n",
    "        embeds = embeds.permute(1, 0, 2)\n",
    "        tfm_out = self.tfm(embeds, length=text_length)\n",
    "        tfm_out_pool = tfm_out.sum(dim=1) / text_length.unsqueeze(1)\n",
    "        logits = self.fc(tfm_out_pool)\n",
    "        scores = F.log_softmax(logits, dim=1)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 하이퍼파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab) # 단어 개수\n",
    "EMBEDDING_DIM = 300 # 임베딩 차원\n",
    "FILTER_SIZE = 600 # 은닉 상태 차원\n",
    "TARGET_SIZE = len(LABEL.vocab.stoi) # 라벨 클래스 개수\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] # 패딩 인덱스\n",
    "NUM_ENC_LAYERS = 2 # 인코더 레이어 개수\n",
    "NUM_HEADS = 1 # 어텐션 헤드 개수\n",
    "DROPOUT_RATE = 0. # 드롭아웃 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4.3 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델, 손실 함수 (loss function), 옵티마이저 (optimizer) 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-73e665bed46a>:13: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  angle_rates = 1 / torch.pow(10000, (2 * (i // 2)) / embedding_dim)\n"
     ]
    }
   ],
   "source": [
    "model = TransformerClassifier(EMBEDDING_DIM, FILTER_SIZE, VOCAB_SIZE, TARGET_SIZE, NUM_ENC_LAYERS, NUM_HEADS, DROPOUT_RATE, PAD_IDX)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임베딩을 사전 학습된 FastText 임베딩으로 덮어씌웁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3327,  0.1853, -0.6197,  ..., -2.1800, -0.2453,  1.4156],\n",
       "        [-0.2287, -1.4974, -1.0487,  ..., -1.0487, -0.7778, -2.1256],\n",
       "        [ 0.9228, -0.0855, -0.3727,  ..., -1.4728, -0.8051,  1.3485],\n",
       "        ...,\n",
       "        [-0.6370,  0.8996, -0.2592,  ...,  0.3337, -0.4391,  0.7791],\n",
       "        [ 0.6624,  0.5354,  1.5111,  ...,  0.0907, -0.3367, -1.2557],\n",
       "        [ 1.2730,  1.9402, -0.9689,  ..., -1.0843,  0.7427,  0.8532]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.word_embeddings.weight.data.copy_(pretrained_embeddings) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사전에 정의되지 않은 단어에 대한 토큰인 `<UNK>`와 빈 칸을 위한 토큰인 `<PAD>`를 0 벡터로 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9228, -0.0855, -0.3727,  ..., -1.4728, -0.8051,  1.3485],\n",
      "        ...,\n",
      "        [-0.6370,  0.8996, -0.2592,  ...,  0.3337, -0.4391,  0.7791],\n",
      "        [ 0.6624,  0.5354,  1.5111,  ...,  0.0907, -0.3367, -1.2557],\n",
      "        [ 1.2730,  1.9402, -0.9689,  ..., -1.0843,  0.7427,  0.8532]])\n"
     ]
    }
   ],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "print(UNK_IDX, PAD_IDX)\n",
    "\n",
    "model.word_embeddings.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.word_embeddings.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(model.word_embeddings.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정확도를 재는 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(prediction, label):\n",
    "    prediction_argmax = prediction.max(dim=-1)[1]\n",
    "    correct = (prediction_argmax == label).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, loss_function):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, text_length = batch.text\n",
    "        if 0. in text_length:\n",
    "            continue\n",
    "        predictions = model(text, text_length)\n",
    "        \n",
    "        loss = loss_function(predictions, batch.label)\n",
    "        acc = accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평가 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, loss_function):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_length = batch.text\n",
    "            if 0. in text_length:\n",
    "                continue\n",
    "            predictions = model(text, text_length)\n",
    "            \n",
    "            loss = loss_function(predictions, batch.label)\n",
    "            acc = accuracy(predictions, batch.label)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "best_valid_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 1.864 | Train Acc: 39.46%\n",
      "\t Val. Loss: 1.381 |  Val. Acc: 50.60%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 1.237 | Train Acc: 59.45%\n",
      "\t Val. Loss: 1.181 |  Val. Acc: 61.27%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 1.053 | Train Acc: 65.57%\n",
      "\t Val. Loss: 1.183 |  Val. Acc: 56.06%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.979 | Train Acc: 67.71%\n",
      "\t Val. Loss: 1.136 |  Val. Acc: 62.53%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.873 | Train Acc: 70.68%\n",
      "\t Val. Loss: 1.119 |  Val. Acc: 62.77%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.800 | Train Acc: 73.21%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 62.91%\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.700 | Train Acc: 76.12%\n",
      "\t Val. Loss: 1.108 |  Val. Acc: 64.53%\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.635 | Train Acc: 78.44%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 63.73%\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.555 | Train Acc: 80.30%\n",
      "\t Val. Loss: 1.171 |  Val. Acc: 63.68%\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.549 | Train Acc: 80.29%\n",
      "\t Val. Loss: 1.228 |  Val. Acc: 64.39%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, loss_function)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, loss_function)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tfm-best.pt')\n",
    "        \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 데이터에 대하여 평가를 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.058 | Test Acc: 63.52%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tfm-best.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, loss_function)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
