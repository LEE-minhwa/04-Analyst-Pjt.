{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [프로젝트2] 토치 기반 모델링 기초 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BG_0zjG13zZy"
   },
   "source": [
    "\n",
    "## 프로젝트 목표\n",
    "---\n",
    "- 딥러닝 프레임워크 중 하나인 PyTorch 사용법 이해\n",
    "- PyTorch를 활용하여 간단한 자연어 딥러닝 모델 제작 및 학습\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pS7RvUWi5vJe"
   },
   "source": [
    "## 프로젝트 목차\n",
    "---\n",
    "\n",
    "1. **딥러닝 모델 구성:** PyTorch를 통하여 기본적인 선형 레이어와 활성화 함수를 만들어봅니다.\n",
    "\n",
    "2. **간단한 딥러닝 모델 제작 및 학습 진행:** PyTorch를 통하여 모델 구성부터 학습까지 진행해봅니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-rxhtJI5_D2"
   },
   "source": [
    "## 프로젝트 개요\n",
    "---\n",
    "\n",
    "프로젝트 1에서 불러오고 자연어 전처리한 데이터를 바탕으로 본격적으로 분류 문제를 딥러닝 모델로 해결해 보고자 합니다.\n",
    "\n",
    "이전에 앞으로 프로젝트에서 사용할 예정인 `Pytorch` 딥러닝 프레임워크 사용법을 익혀봅시다.\n",
    "\n",
    "\n",
    "`Pytorch` 는 tensorflow와 유사한 최신 딥러닝 프레임워크 중 하나입니다. Facebook에서 개발되었으며, 2016년 오픈소스화되었습니다. \n",
    "\n",
    "특히 자연어 처리 분야에서 `Pytorch` 관련 논문의 비율이 앞도적으로 늘어나고 있습니다. \n",
    "\n",
    "따라서, 이번 프로젝트에서는 자연어 처리 분야에서의 `Pytorch` 사용법을 알아볼 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 딥러닝 모델 구성\n",
    "\n",
    "---\n",
    "\n",
    "먼저 토치 라이브러리를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # 신경망 모델 등 모델(레이어)을 만들 때 주로 사용됩니다.\n",
    "import torch.nn.functional as F # 입력/출력값에 대하여 직접적인 계산을 할 때 주로 사용됩니다.\n",
    "import torch.optim as optim # optimizer 관련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1. Fully-connected layer (linear layer)\n",
    "\n",
    "---\n",
    "\n",
    "딥러닝 모델에서 가장 기본적으로 쓰이는 선형 레이어를 만들어 봅니다.\n",
    "\n",
    "$$f(x)=Ax+b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x:  torch.Size([2, 4])\n",
      "shape of f(x):  torch.Size([2, 3])\n",
      "x:  tensor([[ 0.2187,  0.3610, -0.8238,  0.3890],\n",
      "        [ 2.0780, -1.2999, -0.0493,  1.0665]])\n",
      "f(x):  tensor([[-0.1422, -0.9860, -0.2535],\n",
      "        [-0.4592, -0.1136,  0.4756]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "fc = nn.Linear(4, 3) # 4-dim 입력값 (x) 을 3-dim 출력값 (f(x))으로 매핑합니다.\n",
    "x = torch.randn(2, 4) # batch size 2이고 차원이 4인 랜덤 정규분포 벡터를 가져옵니다.\n",
    "fx = fc(x) # fully-connected layer를 태운 출력값이 나옵니다.\n",
    "\n",
    "print('shape of x: ', x.shape)\n",
    "print('shape of f(x): ', fx.shape)\n",
    "print('x: ', x)\n",
    "print('f(x): ', fx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2. Activation function\n",
    "\n",
    "---\n",
    "\n",
    "딥러닝 모델에서 선형 레이어와 함께 쓰여 비선형을 부여해주는 활성화 함수를 사용해 봅시다.\n",
    "\n",
    "$$g(x)=\\sigma(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1937, -1.5797, -0.4643, -0.9161],\n",
      "        [-0.1153, -0.4797, -1.6657, -3.0039]])\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[0.4517, 0.1708, 0.3860, 0.2858],\n",
      "        [0.4712, 0.3823, 0.1590, 0.0473]])\n",
      "tensor([[-0.1913, -0.9186, -0.4336, -0.7240],\n",
      "        [-0.1148, -0.4460, -0.9310, -0.9951]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 4)\n",
    "relu_x = F.relu(x) # ReLU 활성화 함수\n",
    "sigmoid_x = torch.sigmoid(x) # sigmoid 활성화 함수 (F.sigmoid를 사용해도 되나 최근 버전에서는 torch.sigmoid를 쓸 것을 권장합니다.)\n",
    "tanh_x = torch.tanh(x) # tanh 활성화 함수 (F.tanh를 사용해도 되나 최근 버전에서는 torch.tanh를 쓸 것을 권장합니다.)\n",
    "\n",
    "print(x)\n",
    "print(relu_x)\n",
    "print(sigmoid_x)\n",
    "print(tanh_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1937, -1.5797, -0.4643, -0.9161],\n",
      "        [-0.1153, -0.4797, -1.6657, -3.0039]])\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "relu = nn.ReLU() # nn 을 통하여 활성화 함수도 모듈화하여 사용할 수 있습니다.\n",
    "relu_xx = relu(x)\n",
    "\n",
    "print(x)\n",
    "print(relu_xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1937, -1.5797, -0.4643, -0.9161],\n",
      "        [-0.1153, -0.4797, -1.6657, -3.0039]])\n",
      "tensor([[0.4804, 0.2497, 0.7688, 0.8897],\n",
      "        [0.5196, 0.7503, 0.2312, 0.1103]])\n",
      "tensor([[0.4002, 0.1001, 0.3053, 0.1943],\n",
      "        [0.5096, 0.3540, 0.1081, 0.0284]])\n",
      "tensor([[-0.9157, -2.3017, -1.1863, -1.6381],\n",
      "        [-0.6742, -1.0386, -2.2246, -3.5628]])\n",
      "tensor([[-0.9157, -2.3017, -1.1863, -1.6381],\n",
      "        [-0.6742, -1.0386, -2.2246, -3.5628]])\n"
     ]
    }
   ],
   "source": [
    "softmax_x0 = F.softmax(x, dim=0) # 0번째를 기준으로 softmax 함수를 적용합니다.\n",
    "softmax_x1 = F.softmax(x, dim=1) # 1번째를 기준으로 softmax 함수를 적용합니다.\n",
    "\n",
    "log_softmax_x1 = F.log_softmax(x, dim=1) # 손실 함수의 안정성 등을 위하여 log-softmax 출력값을 사용하기도 합니다.\n",
    "\n",
    "print(x)\n",
    "print(softmax_x0)\n",
    "print(softmax_x1)\n",
    "print(torch.log(softmax_x1))\n",
    "print(log_softmax_x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 간단한 딥러닝 모델 제작 및 학습 진행\n",
    "\n",
    "---\n",
    "\n",
    "Bag-of-Words 표현을 입력값으로 받는 간단한 분류 모델을 만들어보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1. Bag-of-Words 표현\n",
    "\n",
    "---\n",
    "\n",
    "자연어 데이터를 표현하는 방법 중 하나로, 각 단어에 인덱스를 부여한 뒤 주어진 데이터에 나타난 단어의 개수를 인덱스에 표현하여 나타낸 벡터입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [(\"I want to eat pizza\".split(), \"ENGLISH\"),\n",
    "              (\"나는 피자를 먹고 싶다\".split(), \"KOREAN\"),\n",
    "              (\"She have to go on a business trip next week\".split(), \"ENGLISH\"),\n",
    "              (\"그녀는 다음 주에 출장을 다녀와야 한다\".split(), \"KOREAN\")]\n",
    "\n",
    "test_data = [(\"I will eat pizza next week\".split(), \"ENGLISH\"),\n",
    "             (\"나는 다음 주에 피자를 먹을 것이다\".split(), \"KOREAN\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 0, 'want': 1, 'to': 2, 'eat': 3, 'pizza': 4, '나는': 5, '피자를': 6, '먹고': 7, '싶다': 8, 'She': 9, 'have': 10, 'go': 11, 'on': 12, 'a': 13, 'business': 14, 'trip': 15, 'next': 16, 'week': 17, '그녀는': 18, '다음': 19, '주에': 20, '출장을': 21, '다녀와야': 22, '한다': 23, 'will': 24, '먹을': 25, '것이다': 26}\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {} # 단어에 인덱스를 부여하는 딕셔너리를 만듭니다.\n",
    "data = train_data + test_data\n",
    "for sent, _ in data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_ix = {\"ENGLISH\": 0, \"KOREAN\": 1} # 라벨에 인덱스를 부여하는 딕셔너리를 만듭니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. 딥러닝 모델 만들기\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(word_to_ix) # 데이터의 총 단어 개수\n",
    "NUM_CLASSES = len(label_to_ix) # 총 클래스 개수\n",
    "VOCAB_SIZE\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module): # 모델을 만들 때 nn.Module을 상속받습니다.\n",
    "    \n",
    "    def __init__(self, num_classes, vocab_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        # 레이어를 정의합니다.\n",
    "        self.fc = nn.Linear(vocab_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 모델에 데이터를 넣었을 때 동작하는 기작을 이 함수에 입력합니다.\n",
    "        x = self.fc(x)\n",
    "        out = F.log_softmax(x, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    # 문장을 bag-of-word 표현으로 나타내는 함수입니다.\n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target(label, label_to_ix):\n",
    "    return torch.LongTensor([label_to_ix[label]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.3. 모델 학습 및 평가\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(NUM_CLASSES, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 파라미터 초기값을 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0973, -0.0640,  0.0943,  0.1337, -0.1912, -0.1743, -0.1360, -0.0103,\n",
      "          0.0417,  0.1136, -0.1618, -0.0251, -0.1848, -0.0497,  0.1415, -0.1445,\n",
      "         -0.1659,  0.1533,  0.0375, -0.0732,  0.0766,  0.1029,  0.0460,  0.1854,\n",
      "          0.1796, -0.1088, -0.0535],\n",
      "        [ 0.1397,  0.1774,  0.1178, -0.0821, -0.0831,  0.0681,  0.1147, -0.0235,\n",
      "         -0.1529, -0.1632, -0.1259,  0.1004, -0.0041,  0.0422, -0.0726, -0.0936,\n",
      "          0.0322, -0.0291, -0.0349, -0.1486, -0.1061,  0.0128,  0.0402,  0.1625,\n",
      "          0.1030, -0.1910, -0.1479]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0834, -0.0784], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 데이터에 대하여 모델의 출력값을 내봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob. of data [I will eat pizza next week] is [[0.48165223 0.5183478 ]].\n",
      "prob. of data [나는 다음 주에 피자를 먹을 것이다] is [[0.48417822 0.51582175]].\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # 그래디언트를 계산하지 않는 모드입니다. (평가할 때 등)\n",
    "    for instance, label in test_data:\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        log_probs = model(bow_vec)\n",
    "        print(f'prob. of data [{\" \".join(instance)}] is {log_probs.exp().numpy()}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실 함수와 옵티마이저를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 학습을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    for instance, label in train_data:\n",
    "        # 기존 모델의 파라미터 별로 계산해놓은 그래디언트를 없앱니다.\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # 데이터를 학습 가능하도록 벡터화 합니다.\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        target = make_target(label, label_to_ix)\n",
    "        \n",
    "        # 데이터를 모델에 넣어 forward pass를 진행합니다.\n",
    "        log_probs = model(bow_vec)\n",
    "        \n",
    "        # 로스를 계산합니다.\n",
    "        loss = loss_function(log_probs, target)\n",
    "        \n",
    "        # 그래디언트를 계산합니다.\n",
    "        loss.backward()\n",
    "        \n",
    "        # 옵티마이저를 통하여 파라미터를 업데이트합니다.\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 평가를 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob. of data [I will eat pizza next week] is [[0.96946967 0.03053034]].\n",
      "prob. of data [나는 다음 주에 피자를 먹을 것이다] is [[0.01835033 0.9816497 ]].\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for instance, label in test_data:\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        log_probs = model(bow_vec)\n",
    "        print(f'prob. of data [{\" \".join(instance)}] is {log_probs.exp().numpy()}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
